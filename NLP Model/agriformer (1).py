# -*- coding: utf-8 -*-
"""AgriFormer.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1aatbHzmvbm0Fv0lpltQWLx8E4HQOD2kZ
"""

!pip install -U sentence-transformers

!python --version

!pip install -U nltk

import json
import re
import nltk
from nltk.stem import WordNetLemmatizer
from sentence_transformers import SentenceTransformer, InputExample, losses
from torch.utils.data import DataLoader

nltk.download('punkt')
nltk.download('wordnet')
nltk.download('omw-1.4')

lemmatizer = WordNetLemmatizer()

from google.colab import files
import json
from sentence_transformers import InputExample

# Upload JSON
uploaded = files.upload()  # Popup se select your AppStructure.json

# Get filename
filename = list(uploaded.keys())[0]

# Load JSON properly
with open(filename, "r") as f:
    data = json.load(f)

# Flatten JSON into list of InputExamples
train_examples = []

for feature in data['features']:
    page_name = feature['page_name']
    for keyword in feature['keywords']:
        # Each keyword becomes a training example
        train_examples.append(InputExample(texts=[keyword], label=page_name))

print(f"Total training examples: {len(train_examples)}")

def clean_text(text):
    text = text.lower()  # lowercase
    text = re.sub(r'[^a-z0-9\s]', '', text)  # remove punctuation
    tokens = nltk.word_tokenize(text)
    tokens = [lemmatizer.lemmatize(t) for t in tokens]  # lemmatize
    return " ".join(tokens)

import nltk
from nltk.stem import WordNetLemmatizer
import re
from sentence_transformers import InputExample

# âœ… Correct NLTK resources
nltk.download('punkt_tab')       # for word_tokenize
nltk.download('wordnet')     # for lemmatizer
nltk.download('omw-1.4')     # for lemmatizer languages

lemmatizer = WordNetLemmatizer()

# Example clean_text function
def clean_text(text):
    text = text.lower()
    text = re.sub(r'[^a-z0-9\s]', '', text)  # remove punctuation
    tokens = nltk.word_tokenize(text)        # uses 'punkt'
    tokens = [lemmatizer.lemmatize(tok) for tok in tokens]
    return ' '.join(tokens)

# Processed examples
processed_examples = []

for feature in data['features']:
    page_name = feature['page_name']
    for keyword in feature['keywords']:
        cleaned_keyword = clean_text(keyword)
        # Each keyword + page_name pair as InputExample
        processed_examples.append(InputExample(texts=[cleaned_keyword, page_name], label=1.0))

print(f"Total processed examples: {len(processed_examples)}")

import os
os.environ["WANDB_DISABLED"] = "true"

# 1. Load a pre-trained agriculture-specific sentence transformer
model = SentenceTransformer('recobo/agri-sentence-transformer')

# 2. Create a DataLoader for training
train_dataloader = DataLoader(processed_examples, shuffle=True, batch_size=16)

# 3. Use CosineSimilarityLoss for keyword-page similarity learning
train_loss = losses.CosineSimilarityLoss(model)

# 4. Fine-tune the model
num_epochs = 10  # Adjust based on dataset size
model.fit(
    train_objectives=[(train_dataloader, train_loss)],
    epochs=num_epochs,
    warmup_steps=50
)

# 5. Save the fine-tuned model
model.save("final_fine_tuned_agri_model")

print("Fine-tuning complete!")

from sentence_transformers import SentenceTransformer
model=SentenceTransformer("final_fine_tuned_agri_model")
test_sentence = "recommend crop to me according to my soil type"

import numpy as np
from sklearn.metrics.pairwise import cosine_similarity

# Flatten all page_name and keywords
candidates = []
candidate_texts = []

for feature in data['features']:
    page_name = feature['page_name']
    for keyword in feature['keywords']:
        candidates.append(page_name)
        candidate_texts.append(keyword)

# Encode candidate keywords
candidate_embeddings = model.encode(candidate_texts)

test_embedding = model.encode([test_sentence])

similarities = cosine_similarity(test_embedding, candidate_embeddings)
best_idx = np.argmax(similarities)
predicted_page = candidates[best_idx]

print(f"Predicted app page: {predicted_page}")

import pickle


# Save it as a pickle file
with open("final.pkl", "wb") as file:  # "wb" = write binary
    pickle.dump(data, file)

print("Pickle file saved successfully!")